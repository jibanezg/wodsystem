/**
 * Browser LLM Provider - Implements LLM interface for browser-based models
 * Uses Transformers.js for client-side model loading and inference
 */

console.log('BrowserLLMProvider: File is being loaded...');

class BrowserLLMProvider {
    constructor(modelConfig = {}) {
        this.model = null;
        this.tokenizer = null;
        this.pipeline = null;
        this.isLoading = false;
        this.loadProgress = 0;
        this.transformersLoaded = false;
        
        // Default configuration
        this.config = {
            modelName: modelConfig.modelName || 'microsoft/DialoGPT-medium',
            modelPath: modelConfig.modelPath || null,
            quantized: true,
            progressCallback: modelConfig.progressCallback || null,
            ...modelConfig
        };
    }

    /**
     * Load Transformers.js from CDN
     */
    async loadTransformers() {
        if (this.transformersLoaded && window.transformersPipeline) {
            return window.transformersPipeline;
        }

        // Check if Transformers.js is already loaded globally
        if (window.transformers && window.transformers.pipeline) {
            this.transformersLoaded = true;
            window.transformersPipeline = window.transformers.pipeline;
            console.log('LLM: Transformers.js found globally');
            return window.transformersPipeline;
        }

        console.error('LLM: Transformers.js not found. Please add the script tag to your HTML.');
        console.error('Add: <script src="https://cdn.jsdelivr.net/npm/@xenova/transformers@2.6.0/dist/transformers.min.js"></script>');
        
        throw new Error('Transformers.js library not found. Please include the Transformers.js CDN script in your HTML page.');
    }

    /**
     * Initialize the browser LLM provider
     * Loads the model and tokenizer using Transformers.js
     */
    async initialize() {
        if (this.pipeline) {
            console.log('LLM: Already initialized');
            return;
        }

        this.isLoading = true;
        this.loadProgress = 0;

        try {
            console.log('LLM: Starting initialization...');
            
            // Load Transformers.js if not already loaded
            const pipeline = await this.loadTransformers();
            
            console.log('LLM: Loading model:', this.config.modelName);
            
            // Load the pipeline
            this.pipeline = await pipeline('text-generation', this.config.modelName, {
                quantized: this.config.quantized,
                progress_callback: (progress) => {
                    this.loadProgress = progress;
                    if (this.config.progressCallback) {
                        this.config.progressCallback(progress);
                    }
                }
            });

            this.isLoading = false;
            this.loadProgress = 100;
            console.log('LLM: Model loaded successfully');
            
        } catch (error) {
            this.isLoading = false;
            console.error('LLM: Initialization failed:', error.message);
            throw error;
        }
    }

    /**
     * Generate text using the loaded model
     * @param {string} prompt - The prompt to send to the model
     * @param {Object} parameters - Generation parameters
     * @returns {string} Generated text response
     */
    async generate(prompt, parameters = {}) {
        if (!this.pipeline) {
            throw new Error('Model not loaded. Call initialize() first.');
        }

        try {
            console.log('LLM: Generating response...');
            
            // Prepare generation parameters
            const generationParams = {
                max_new_tokens: parameters.maxTokens || 2048,
                temperature: parameters.temperature || 0.1,
                do_sample: true,
                top_k: parameters.topK || 50,
                top_p: parameters.topP || 0.9,
                repetition_penalty: parameters.repetitionPenalty || 1.1,
                pad_token_id: this.pipeline.tokenizer.eos_token_id
            };

            // Generate response
            const result = await this.pipeline(prompt, generationParams);
            
            // Extract generated text
            const generatedText = result[0].generated_text;
            
            // Remove the original prompt from the response
            const response = generatedText.substring(prompt.length).trim();
            
            console.log('LLM: Generation complete');
            return response;
            
        } catch (error) {
            console.error('LLM: Generation failed:', error.message);
            throw error;
        }
    }

    /**
     * Check if the environment supports required features
     */
    checkEnvironmentSupport() {
        const issues = [];
        
        // Check for dynamic import support
        if (typeof import !== 'function') {
            issues.push('Dynamic imports not supported');
        }
        
        // Check for fetch support (needed for CDN loading)
        if (typeof fetch !== 'function') {
            issues.push('Fetch API not supported');
        }
        
        // Check for Web Workers support (needed by Transformers.js)
        if (typeof Worker === 'undefined') {
            issues.push('Web Workers not supported');
        }
        
        // Check for SharedArrayBuffer support (needed for some models)
        if (typeof SharedArrayBuffer === 'undefined') {
            issues.push('SharedArrayBuffer not supported (may affect model loading)');
        }
        
        return {
            supported: issues.length === 0,
            issues: issues
        };
    }

    /**
     * Get provider status and information
     */
    getStatus() {
        const envCheck = this.checkEnvironmentSupport();
        
        return {
            modelName: this.config.modelName,
            isLoaded: !!this.pipeline,
            isLoading: this.isLoading,
            loadProgress: this.loadProgress,
            transformersLoaded: this.transformersLoaded,
            environmentSupported: envCheck.supported,
            environmentIssues: envCheck.issues,
            config: this.config
        };
    }

    /**
     * Unload the model to free memory
     */
    async unload() {
        if (this.pipeline) {
            // Clean up pipeline
            this.pipeline = null;
            this.model = null;
            this.tokenizer = null;
            console.log('LLM: Model unloaded');
        }
    }

    /**
     * Update configuration
     * @param {Object} newConfig - New configuration parameters
     */
    updateConfig(newConfig) {
        this.config = { ...this.config, ...newConfig };
        console.log('LLM: Configuration updated');
    }
}

// Export for use in other modules
window.BrowserLLMProvider = BrowserLLMProvider;
console.log('BrowserLLMProvider: File loaded and exported to window.BrowserLLMProvider'); 